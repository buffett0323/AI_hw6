
Using cuda device
==((====))==  Unsloth: Fast Mistral patching release 2024.5
   \\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.684 GB. Platform = Linux.
O^O/ \_/ \    Pytorch: 2.3.0. CUDA = 8.6. CUDA Toolkit = 12.1.
\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Traceback (most recent call last):
  File "/home/ddmanddman/buffett_use/AI_hw6/main.py", line 81, in <module>
    DPO.DPO_train(args, output_dir)
  File "/home/ddmanddman/buffett_use/AI_hw6/DPO.py", line 38, in DPO_train
    model, tokenizer = FastLanguageModel.from_pretrained(
  File "/home/ddmanddman/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/loader.py", line 142, in from_pretrained
    model, tokenizer = dispatch_model.from_pretrained(
  File "/home/ddmanddman/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/mistral.py", line 366, in from_pretrained
    model = AutoModelForCausalLM.from_pretrained(
  File "/home/ddmanddman/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 563, in from_pretrained
    return model_class.from_pretrained(
  File "/home/ddmanddman/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3754, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/home/ddmanddman/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4214, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/home/ddmanddman/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 889, in _load_state_dict_into_meta_model
    hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)
  File "/home/ddmanddman/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 201, in create_quantized_param
    new_value = bnb.nn.Params4bit.from_prequantized(
  File "/home/ddmanddman/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 278, in from_prequantized
    self = torch.Tensor._make_subclass(cls, data.to(device))
KeyboardInterrupt